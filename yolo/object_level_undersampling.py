#!/usr/bin/env python3
"""
ê°ì²´ ë‹¨ìœ„ ì–¸ë”ìƒ˜í”Œë§ (ì „ëµ A)
ê° ì´ë¯¸ì§€ì—ì„œ resistor/capacitor ê°ì²´ë¥¼ ëœë¤í•˜ê²Œ ì¼ë¶€ë§Œ ì„ íƒ
"""

import os
import random
import yaml
from pathlib import Path
from collections import defaultdict
from tqdm import tqdm

class ObjectLevelUndersampler:
    def __init__(self):
        self.base_dir = Path("/home/sys1041/work_project/data/processed/balanced_pcb_model")
        self.output_dir = Path("/home/sys1041/work_project/data/processed/final_balanced_pcb_model")

        # ê°ì²´ ë‹¨ìœ„ ì–¸ë”ìƒ˜í”Œë§ ëŒ€ìƒ (í˜„ì¬ ë°ì´í„°ì…‹ì˜ class ID)
        self.target_classes = {
            19: 5000,  # comp_resistor: 26,113 â†’ 5,000
            10: 5000,  # comp_capacitor: 23,254 â†’ 5,000
        }

        random.seed(42)

    def calculate_per_image_limit(self, split):
        """ê° ì´ë¯¸ì§€ë‹¹ ìœ ì§€í•  ê°ì²´ ìˆ˜ ê³„ì‚°"""
        labels_dir = self.base_dir / split / 'labels'

        # í´ë˜ìŠ¤ë³„ ì´ ê°ì²´ ìˆ˜ ë° ì´ë¯¸ì§€ë‹¹ ê°ì²´ ìˆ˜ ì¹´ìš´íŠ¸
        class_stats = defaultdict(lambda: {'total': 0, 'images': defaultdict(int)})

        print(f"  ë¶„ì„ ì¤‘: {split} setì˜ í´ë˜ìŠ¤ë³„ ë¶„í¬...")
        for label_file in labels_dir.glob('*.txt'):
            with open(label_file, 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) >= 5:
                        class_id = int(parts[0])
                        class_stats[class_id]['total'] += 1
                        class_stats[class_id]['images'][label_file.stem] += 1

        # ê° í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ë‹¹ í‰ê·  ìœ ì§€ ë¹„ìœ¨ ê³„ì‚°
        sampling_ratios = {}
        for class_id, target_count in self.target_classes.items():
            current_total = class_stats[class_id]['total']
            num_images = len(class_stats[class_id]['images'])

            if current_total > target_count:
                # ì „ì²´ ë¹„ìœ¨ë¡œ ê³„ì‚°
                ratio = target_count / current_total
                sampling_ratios[class_id] = ratio
                print(f"    í´ë˜ìŠ¤ {class_id}: {current_total:,}ê°œ â†’ {target_count:,}ê°œ (ë¹„ìœ¨: {ratio:.2%})")
            else:
                sampling_ratios[class_id] = 1.0
                print(f"    í´ë˜ìŠ¤ {class_id}: {current_total:,}ê°œ (ìœ ì§€)")

        return sampling_ratios

    def process_split(self, split):
        """Train/Val/Test ê° split ì²˜ë¦¬"""
        print(f"\n{'='*80}")
        print(f"ì²˜ë¦¬ ì¤‘: {split} set")
        print(f"{'='*80}")

        # ìƒ˜í”Œë§ ë¹„ìœ¨ ê³„ì‚°
        sampling_ratios = self.calculate_per_image_limit(split)

        images_dir = self.base_dir / split / 'images'
        labels_dir = self.base_dir / split / 'labels'

        output_images_dir = self.output_dir / split / 'images'
        output_labels_dir = self.output_dir / split / 'labels'

        output_images_dir.mkdir(parents=True, exist_ok=True)
        output_labels_dir.mkdir(parents=True, exist_ok=True)

        # í†µê³„
        stats = defaultdict(lambda: {'before': 0, 'after': 0})
        processed_images = 0

        print(f"\n  ì²˜ë¦¬ ì¤‘: ë¼ë²¨ íŒŒì¼ ì–¸ë”ìƒ˜í”Œë§...")

        for label_file in tqdm(list(labels_dir.glob('*.txt'))):
            image_name = label_file.stem
            image_file = images_dir / f"{image_name}.jpg"

            if not image_file.exists():
                image_file = images_dir / f"{image_name}.png"

            if not image_file.exists():
                continue

            # ë¼ë²¨ ì½ê¸° ë° í´ë˜ìŠ¤ë³„ ë¶„ë¥˜
            lines_by_class = defaultdict(list)

            with open(label_file, 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) >= 5:
                        class_id = int(parts[0])
                        lines_by_class[class_id].append(line)
                        stats[class_id]['before'] += 1

            # ìƒˆ ë¼ë²¨ ìƒì„±
            new_lines = []

            for class_id, lines in lines_by_class.items():
                if class_id in self.target_classes:
                    # ì–¸ë”ìƒ˜í”Œë§ ì ìš©
                    ratio = sampling_ratios.get(class_id, 1.0)
                    num_to_keep = max(1, int(len(lines) * ratio))  # ìµœì†Œ 1ê°œëŠ” ìœ ì§€

                    if num_to_keep < len(lines):
                        selected_lines = random.sample(lines, num_to_keep)
                    else:
                        selected_lines = lines

                    new_lines.extend(selected_lines)
                    stats[class_id]['after'] += len(selected_lines)
                else:
                    # ë‹¤ë¥¸ í´ë˜ìŠ¤ëŠ” ëª¨ë‘ ìœ ì§€
                    new_lines.extend(lines)
                    stats[class_id]['after'] += len(lines)

            # íŒŒì¼ ì €ì¥
            if new_lines:
                # ì´ë¯¸ì§€ ë³µì‚¬ (ì‹¬ë³¼ë¦­ ë§í¬ë¡œ ê³µê°„ ì ˆì•½)
                import shutil
                shutil.copy2(image_file, output_images_dir / image_file.name)

                # ë¼ë²¨ ì €ì¥
                with open(output_labels_dir / label_file.name, 'w') as f:
                    f.writelines(new_lines)

                processed_images += 1

        print(f"\n  ì™„ë£Œ: {processed_images:,}ê°œ ì´ë¯¸ì§€ ì²˜ë¦¬")

        # í´ë˜ìŠ¤ë³„ í†µê³„ ì¶œë ¥
        print(f"\n  í´ë˜ìŠ¤ë³„ ê°ì²´ ìˆ˜ ë³€í™”:")
        for class_id in sorted(stats.keys()):
            before = stats[class_id]['before']
            after = stats[class_id]['after']
            diff = before - after
            if before > 0:
                reduction = (1 - after/before) * 100
                print(f"    í´ë˜ìŠ¤ {class_id:2d}: {before:6,}ê°œ â†’ {after:6,}ê°œ (-{diff:6,}ê°œ, -{reduction:5.1f}%)")

        return stats

    def analyze_final_distribution(self):
        """ìµœì¢… ë°ì´í„°ì…‹ ë¶„í¬ ë¶„ì„"""
        print("\n" + "="*80)
        print("ìµœì¢… ë°ì´í„°ì…‹ ë¶ˆê· í˜• ë¶„ì„")
        print("="*80)

        for split in ['train', 'val', 'test']:
            labels_dir = self.output_dir / split / 'labels'

            class_counts = defaultdict(int)
            total_images = 0
            total_objects = 0

            for label_file in labels_dir.glob('*.txt'):
                total_images += 1
                with open(label_file, 'r') as f:
                    for line in f:
                        parts = line.strip().split()
                        if len(parts) >= 5:
                            class_id = int(parts[0])
                            class_counts[class_id] += 1
                            total_objects += 1

            print(f"\n{split.upper()} SET:")
            print(f"  ì´ ì´ë¯¸ì§€: {total_images:,}ê°œ")
            print(f"  ì´ ê°ì²´: {total_objects:,}ê°œ")
            print(f"  í‰ê·  ê°ì²´/ì´ë¯¸ì§€: {total_objects/total_images if total_images > 0 else 0:.2f}")

            if class_counts:
                # ë¶ˆê· í˜• ë¹„ìœ¨
                max_count = max(class_counts.values())
                min_count = min(class_counts.values())
                avg_count = sum(class_counts.values()) / len(class_counts)

                print(f"\n  ğŸ“Š ë¶ˆê· í˜• í†µê³„:")
                print(f"    ìµœëŒ€ ìƒ˜í”Œ: {max_count:,}ê°œ")
                print(f"    ìµœì†Œ ìƒ˜í”Œ: {min_count:,}ê°œ")
                print(f"    í‰ê·  ìƒ˜í”Œ: {avg_count:,.0f}ê°œ")
                print(f"    ë¶ˆê· í˜• ë¹„ìœ¨: {max_count/min_count:.1f}:1")

                # ìƒìœ„/í•˜ìœ„ í´ë˜ìŠ¤
                sorted_classes = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)

                print(f"\n  ìƒìœ„ 5ê°œ í´ë˜ìŠ¤:")
                for class_id, count in sorted_classes[:5]:
                    print(f"    í´ë˜ìŠ¤ {class_id:2d}: {count:6,}ê°œ")

                print(f"\n  í•˜ìœ„ 5ê°œ í´ë˜ìŠ¤:")
                for class_id, count in sorted_classes[-5:]:
                    print(f"    í´ë˜ìŠ¤ {class_id:2d}: {count:6,}ê°œ")

    def create_data_yaml(self):
        """data.yaml ë³µì‚¬"""
        print("\ndata.yaml ë³µì‚¬ ì¤‘...")

        import shutil
        src_yaml = self.base_dir / 'data.yaml'
        dst_yaml = self.output_dir / 'data.yaml'

        # ê²½ë¡œë§Œ ìˆ˜ì •í•´ì„œ ë³µì‚¬
        with open(src_yaml, 'r') as f:
            data = yaml.safe_load(f)

        data['path'] = str(self.output_dir.absolute())

        with open(dst_yaml, 'w') as f:
            yaml.dump(data, f, default_flow_style=False, sort_keys=False)

        print(f"  ì €ì¥ ì™„ë£Œ: {dst_yaml}")

    def run(self):
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("="*80)
        print("ê°ì²´ ë‹¨ìœ„ ì–¸ë”ìƒ˜í”Œë§ ì‹œì‘ (ì „ëµ A)")
        print("="*80)
        print(f"\nì…ë ¥: {self.base_dir}")
        print(f"ì¶œë ¥: {self.output_dir}")
        print(f"\nëª©í‘œ:")
        for class_id, target in self.target_classes.items():
            print(f"  í´ë˜ìŠ¤ {class_id}: {target:,}ê°œë¡œ ì œí•œ")

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # ê° split ì²˜ë¦¬
        all_stats = {}
        for split in ['train', 'val', 'test']:
            all_stats[split] = self.process_split(split)

        # data.yaml ë³µì‚¬
        self.create_data_yaml()

        # ìµœì¢… í†µê³„
        self.analyze_final_distribution()

        print("\n" + "="*80)
        print("ê°ì²´ ë‹¨ìœ„ ì–¸ë”ìƒ˜í”Œë§ ì™„ë£Œ!")
        print("="*80)
        print(f"\nìƒˆ ë°ì´í„°ì…‹ ê²½ë¡œ: {self.output_dir}")
        print(f"ìƒˆ data.yaml: {self.output_dir / 'data.yaml'}")

if __name__ == "__main__":
    undersampler = ObjectLevelUndersampler()
    undersampler.run()
